**Google Cloud Composer PySpark Word Count**

Welcome to the Google Cloud Composer PySpark Word Count example project. This project demonstrates the orchestration of a PySpark Word Count job using Google Cloud Composer. The purpose of this project is to showcase how to create, configure, and run a Cloud Dataproc cluster, execute a PySpark job, and manage the cluster's lifecycle within Google Cloud Composer.

**Project Details**

**Project Name**: Pyspark Project
**Project ID**: yspark-project-400006
**Location**: us-east1

**Project Structure
**

The project consists of the following components:

1. **DAGs**: This folder contains the Directed Acyclic Graph (DAG) definition, composer_pyspark_wordcount.py, which orchestrates the PySpark Word Count job. It schedules the creation of a Cloud Dataproc cluster, execution of the PySpark job, and deletion of the cluster upon job completion.
2. **Data**: This directory is where your input data is stored, and it contains the Shakespeare text file (rose.txt) used for the Word Count job.
3. **Logs**: Logs generated by Google Cloud Composer and the Dataproc job are stored in this directory.

**PySpark Word Count Job
**

The core of this project is the PySpark Word Count job. This job processes the **rose.txt** file, counts the occurrences of each word, and stores the results in a designated Cloud Storage bucket.

**Usage**

This project serves as a reference for setting up and running similar data processing workflows using Google Cloud Composer. While it doesn't provide step-by-step setup instructions, you can use the provided DAG definition **(composer_pyspark_wordcount.py)** and adapt it to your specific use case.

For detailed setup instructions and configuration options, refer to the official Google Cloud Composer documentation: https://cloud.google.com/composer/docs

**Prerequisites**

Before running this project, ensure you have the following prerequisites:

* A Google Cloud Platform (GCP) account with the necessary permissions.
* Google Cloud Composer and Google Cloud Dataproc services enabled in your GCP project.
* Properly configured credentials for authentication.
* The input data file (rose.txt) should be available in a Google Cloud Storage bucket, and the path should be updated in the DAG configuration.
